[
  {
    "name": "softmax",
    "args": [
      {
        "name": "x"
      },
      {
        "name": "axis"
      }
    ],
    "docstring": "Softmax activation function.\n\n    # Arguments\n        x : Tensor.\n        axis: Integer, axis along which the softmax normalization is applied.\n\n    # Returns\n        Tensor, output of softmax transformation.\n\n    # Raises\n        ValueError: In case `dim(x) == 1`.\n    "
  },
  {
    "name": "elu",
    "args": [
      {
        "name": "x"
      },
      {
        "name": "alpha"
      }
    ]
  },
  {
    "name": "selu",
    "args": [
      {
        "name": "x"
      }
    ],
    "docstring": "Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n\n    # Arguments\n        x: A tensor or variable to compute the activation function for.\n\n    # References\n        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n    "
  },
  {
    "name": "softplus",
    "args": [
      {
        "name": "x"
      }
    ]
  },
  {
    "name": "softsign",
    "args": [
      {
        "name": "x"
      }
    ]
  },
  {
    "name": "relu",
    "args": [
      {
        "name": "x"
      },
      {
        "name": "alpha"
      },
      {
        "name": "max_value"
      }
    ]
  },
  {
    "name": "tanh",
    "args": [
      {
        "name": "x"
      }
    ]
  },
  {
    "name": "sigmoid",
    "args": [
      {
        "name": "x"
      }
    ]
  },
  {
    "name": "hard_sigmoid",
    "args": [
      {
        "name": "x"
      }
    ]
  },
  {
    "name": "linear",
    "args": [
      {
        "name": "x"
      }
    ]
  }
]